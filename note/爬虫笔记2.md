### 一、发送带请求头的请求

**header的形式：字典**

```python
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36"}
```

**用法**

```python
requests.get(url, headers=headers)
```

### 二、发送post请求

**请求参数的形式：字典**

```python
kw = {'wd':'长城'}
```

**参数的用法**

```python
requests.get(url,params=kw)
```

**带参请求的两种方式**

- 利用params参数发送带参数的请求
- 直接发送带参数的url的请求参数

### 三、代理

**代理的过程**

![enter description here](./images/TuringEmmy201811181542507992.png "TuringEmmy201811181542507992")

**正向代理和反向代理的区别**

![enter description here](./images/TuringEmmy201811181542508064.png "TuringEmmy201811181542508064")


- 正向代理：对于浏览器知道服务器的真实地址，例如VPN
- 反向代理：浏览器不知道服务器的真实地址，例如nginx

### 四、代理ip

- 用法：

  ```
    requests.get("http://www.baidu.com",  proxies = proxies)

  ```

- proxies的形式：字典

- 例如：

  ```
    proxies = { 
        "http": "http://12.34.56.79:9527", 
        "https": "https://12.34.56.79:9527", 
        }
  ```

**代理ip的分类**

| 分类                                     | 说明                               |
| -------------------------------------- | -------------------------------- |
| 透明代理(Transparent Proxy)                | 透明代理虽然可以直接“隐藏”你的IP地址，但是还是可以查到你是谁 |
| 匿名代理(Anonymous Proxy)                  | 使用匿名代理，别人只能知道你用了代理，无法知道你是谁       |
| 高匿代理(Elite proxy或High Anonymity Proxy) | 高匿代理让别人根本无法发现你是在用代理，所以是最好的选择。    |

**从请求使用的协议可以分为:**

- http代理
- https代理
- socket代理等

### 五、使用cookies直接访问登陆后的页面

**在headers中使用cookie**

```python
headers = {
"User-Agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36",
"Cookie":" Pycharm-26c2d973=dbb9b300-2483-478f-9f5a-16ca4580177e; Hm_lvt_98b9d8c2fd6608d564bf2ac2ae642948=1512607763; Pycharm-26c2d974=f645329f-338e-486c-82c2-29e2a0205c74; _xsrf=2|d1a3d8ea|c5b07851cbce048bd5453846445de19d|1522379036"}

requests.get(url,headers=headers)
```

> cookie有过期时间 ，所以直接复制浏览器中的cookie可能意味着下一程序继续运行的时候需要替换代码中的cookie，对应的我们也可以通过一个程序专门来获取cookie供其他程序使用；当然也有很多网站的cookie过期时间很长，这种情况下，直接复制cookie来使用更加简单

**cookies参数接受字典形式的cookie**

```python
cookies = {"cookie的name":"cookie的value"}
requests.get(url,headers=headers,cookies=cookie_dict}
```

### 六、session的使用

requests 提供了一个叫做session类，来实现客户端和服务端的`会话保持`

会话保持有两个内涵：

- 保存cookie，下一次请求会带上前一次的cookie
- 实现和服务端的长连接，加快请求速度

```python
session = requests.session()
response = session.get(url,headers)
```

### 七、cookiejar的转换

> response.cookies是CookieJar类型
>
> 使用requests.utils.dict_from_cookiejar，能够实现把cookiejar对象转化为字典

```python
cookies = requests.utils.dict_from_cookiejar(response.cookies)
```

### 八、verify忽略安全认证

```
ssl.CertificateError ...
```

面对上面这暗中证书错误的

```python
response = requests.get(url,verify=False)

# 这个代码不常用，位置的放置也很重要
requests.packages.urllib3.disable_warnings() # 不显示安全提示 
```

### 九、超时参数

```
response = requests.get(url,timeout=3)

```

通过添加timeout参数，能够保证在3秒钟内返回响应，否则会报错

> 这个方法还能够拿来检测代理ip的质量，如果一个代理ip在很长时间没有响应，那么添加超时之后也会报错，对应的这个ip就可以从代理ip池中删除
>
> 使用超时参数能够加快我们整体的请求速度，但是在正常的网页浏览过成功，如果发生速度很慢的情况，我们会做的选择是**刷新页面**，那么在代码中，我们是否也可以刷新请求呢？

**retrying和requests的简单封装**

实现一个发送请求的函数，每次爬虫中直接调用该函数即可实现发送请求，在其中

- 使用timeout实现超时报错
- 使用retrying模块实现重试

```python
#最大重试3次，3次全部报错，才会报错
@retry(stop_max_attempt_number=3) 
def _parse_url(url)
    #超时的时候回报错并重试
    response = requests.get(url, headers=headers, timeout=3) 
    #状态码不是200，也会报错并重试
    assert response.status_code == 200
    return response


def parse_url(url)
    try: #进行异常捕获
        response = _parse_url(url)
    except Exception as e:
        print(e)
        #报错返回None
        response = None
    return response
```