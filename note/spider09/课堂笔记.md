1. 翻页请求的思路
```
找到下一页的URL地址
构造url地址的请求对象，传递给引擎
```
2. 构造Request对象
```
在解析函数中构造request
      request = scrapy.Request(url,callback)
      callback指定request对应的response的解析函数
```
3. tencent爬虫
```
last_page : href = javascript:;
      next_page : text = 下一页
      tr_list --> a/text()
      4 scrapy.Request的更多参数
      scrapy.Request(url[,callback,method="GET",headers,body,cookies,meta,dont_filter=False])

```
4. meta参数：meta可以实现数据在不同的解析函数中的传递
```
response.meta = request.meta 
# 把request的meta属性给response
# meta参数是一个字典
# 在A函数中构造request对象，加入meta参数，callback指向B函数
# 在B函数中通过response.meta取出传递的数据
def A(response):
	yield scrapy.Request(url, callback=B, meta={'item': 'xxxx'}) # 传入
def B(response):
	xxxx = response.meta['item'] # 取出
```

5. BaseItem的使用
```
定义字段 防止手误；对扩展组件做支撑；增加功能；格式化展示日志中的数据
       title = scrapy.Field()
       在爬虫中导入并且实例化
```

6. scrapy的模拟登陆
```
直接携带cookies	
# scrapy.Spider.start_requests函数：
  # 专门对start_url构造request
  # start_url不会过滤去重
  # 某个起始的url需要登录后才能获取，需要重写start_requests方法！构造request时，添加cookies！
  request.cookies 接收一个字典

找url地址，发送post请求自动做状态保持！
  # scrapy能够帮我们自动处理cookie的传递！
  # scrapy能够帮我们自动发送重定向的请求！
  # 必须指定callback才能传递cookie!!!
```

7. 管道
```
 常用方法
 process_item（item, spider）
 # 最后必须return item
 open_spider（spider）
 close_spider（spider）
```