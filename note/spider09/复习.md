1. pymongo模块
```
from pymongo import MongoClient
   # client = MongoClient()
   uri = 'mongodb://账号:密码@127.0.0.1'
   client = MongoClient(uri, port=27017)
   col = client.库名.集合名

   col.insert({}/[{},...])
   col.find_one({})
   col.find({})
   col.delete_one({})
   col.delete_many({})
   col.update({条件},
   		   {'$set':{指定的kv}},
   		   multi=True, #更新多条
   		   upsert=True) #有就更新无就插入
```

2. scrapy框架简单使用
```
 scrapy startproject 项目名
   scrapy genspider 爬虫名 范围域名
   scrapy crawl 爬虫名
```

3. spider.py爬虫
```
class Spider(scrapy.Spider):
   	name = 爬虫名
   	allowed_domains=['范围域名', '可多个']
   	start_urls=['起始url','可多个','不受allowed_domains限制']
   	# scrapy.Spider爬虫类必须有名为parse的解析函数
   	def parse(self, response):
   		...
   		# 传递给引擎
   		yield item
```

4. 数据提取的方法
```
s_list = response.xpath('xpath_str') # 类list
   s_list.extract() # 字符串构成list
   s_list.extract_first() # list中第一个字符串
```

5. response的常用属性
```
response.url
   response.request.url
   response.headers
   response.request.headers
   response.status
   response.body

```
6. 管道需要在settings.py中开启管道的配置