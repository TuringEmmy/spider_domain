1. 构造request对象
```
scrapy.Request(url, 
   			   callback=func, # 指定的解析函数
   			   headers={没有cookies},
   			   cookies={},
   			   method='GET',
   			   body=, # post_data
   			   meta=, # 向func函数中传递数据，dict
   			   dont_filter=False) # 默认False表示过滤重复请求

   # 专门发送post请求
   scrapy.FormRequest(url,
   				   # 没有method、body参数
   				   formdata={post_data}
   				   # 其他参数和Request一样)
```

2. scrapy框架可以自动传递cookie(前提是指定callback，向callback函数中构造request传递)
```
scrapy框架能够自动发送重定向的请求
   起始url需要cookie时，重写该函数
   带上cookies
```

3. request.meta 、response.meta 的使用
```
在构造request对象时，meta接收一个字典:
   	scrapy.Request(url, callback=func, meta={'item':item})
   在引擎中传递给response：
   	response.meta = request.meta
   在request.callback指定的解析函数中可以取出meta中传递的数据：
   	item = response.meta['item']
```

4. items.py中BaseItem类
```
# 爬虫中解析函数只能yield {}、BaseItem、Request、None
   # BaseItem本质就是一个字典，提前定义字段，对一些扩展组件支持
   字段1 = scrapy.Field() 
   # 在爬虫中需要导入并实例化BaseItem类，之后像字典一样使用
```

5. pipelines.py中管道的使用
```
 # 在settings.py中设置开启管道
   	左边是位置，右边是权重(int)
   	权重值越小，越优先执行！
   # 管道类的常用函数
   	process_item(item, spider)
   		# 管道类中必有该函数
   		# 最后一定要return item，其他权重较低的管道类的同名函数才能够接收到item数据
   		# 每当spider中yiled一个item时，该函数就会被调用
   		# spider参数是爬虫的类对象，可以判断spider.name来指定爬虫使用固定的管道
   	open_spider(spider)
   		# spider.name来指定爬虫使用固定的管道
   		# 当爬虫启动时仅执行一次
   		# 一般用来创建文件对象，数据库连接对象
   	close_spider(spider)
   		# spider.name来指定爬虫使用固定的管道
   		# 当爬虫关闭时仅执行一次
   		# 一般用来关闭文件对象，数据库连接对象

   # 多个爬虫可以共用一个管道
   # 一个爬虫可以使用多个管道
```