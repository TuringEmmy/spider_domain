1. pymongo模块
   ```
   from pymongo import MongoClient
   # 实例化连接对象
   # client = MongoClient(host='127.0.0.1', port=27017)
   uri = 'mongodb://账号:密码@127.0.0.1'
   client = MongoClient(uri, port=27017)
   # 创建集合操作对象，无需提前建库建集合
   col = client.数据库名.集合名
   # col = client['数据库名']['集合名']

   col.insert({一条数据}/[{}, {},...]) # 插入一条/多条数据
   col.find_one({条件}) # 返回{} 
   col.find() # 返回只能遍历一次的cursor游标对象，多条数据
   col.delete_one({条件})
   col.delete_many({条件})
   col.update({条件},
   		   {'$set':{指定的kv}},
   		   multi=False/True, # 默认False表示更新一条
   		   upsert=False/True # 默认False，True表示存在就修改，不存在就插入
   		   )
   ```

2. scrapy框架使用


   ```
# 利用了twsited异步网络框架
	# 异步和非阻塞区别：异步是过程，非阻塞强调的是状态
	a. 创建scrapy项目
		scrapy startproject 项目名
	b. 在项目路径下创建爬虫
		scrapy genspider 爬虫名 爬取范围的域名
	c. 完善爬虫等模块
	d. 在项目路径下运行爬虫
		scrapy crawl 爬虫名
   ```


3. spider.py爬虫模块
```
   class Spider(scrapy.Spider)： # 继承父类是scrapy.Spider
   	name = 爬虫名
   	allowed_domains = ['爬取范围的域名', '可以是多个']
   	start_urls = ['起始的url', '可以是多个']

   	# scrapy.Spider类爬虫必须有名为parse的解析函数
   	def parse(self, response):
   		# 专门解析start_url对应的响应内容
   		# response参数就是每个start_url的响应对象
   		yield item # {}, BaseItem, request, None
```

4. response对象常用的属性
```
   response.url # 响应的url
   response.request.url # 响应对应的请求的url
   response.headers # 响应的头
   response.request.headers # 响应对应的请求的头
   response.body # 响应的内容 bytes
   response.status # 响应的状态码
```

5. 提取数据的方法
```
   div_list = response.xpath(xpath_str)
   # 返回由selector对象构成的类list对象
   for div in div_list:
   	div.xpath(xpath_str).extract() # 返回列表中全部字符串
   	div.xpath(xpath_str).extract_first() # 返回列表中第一个字符串
```

6. pipelines.py管道使用，不要忘记在settings.py中开启管道类

7. scrapy的352阵容
```
   三个内置数据对象：request response item/BaseItem
   五个模块：
   	scheduler调度器
   	downloader下载器
   	spider爬虫
   	pipeline管道
   	engine引擎
   俩个中间件：
   	爬虫中间件
   	下载中间件
```

8. scrapy的工作流程
```
   a. spider中对start_url构造request
   b. request--爬虫中间件--引擎--调度器，放入请求队列
   c. 调度器从队列取出request--引擎--下载中间件--下载器，发送请求获取response
   d. response--下载中间件--引擎--爬虫中间件--爬虫
   e. 爬虫对response提取，提取url构造request，重复b步骤
   f. 爬虫对response提取，提取item--引擎--管道，处理保存
```
