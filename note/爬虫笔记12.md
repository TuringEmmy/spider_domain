### 一、scrapy的日志信息

**scrapy的日志信息**

### 二、scrapy的配置文件

**常用常用的配置**

| 配置                      | 说明                                       |
| ----------------------- | ---------------------------------------- |
| ROBOTSTXT_OBEY          | 是否遵守robots协议，默认是遵守                       |
| USER_AGENT              | 用户代理                                     |
| DEFAULT_REQUEST_HEADERS | 设置默认请求头，这里加入了USER_AGENT将不起作用             |
| ITEM_PIPELINES          | 管道，左位置右权重：权重值越小，越优先执行                    |
| SPIDER_MIDDLEWARES      | 爬虫中间件，设置过程和管道相同                          |
| DOWNLOADER_MIDDLEWARES  | 下载中间件                                    |
| COOKIES_ENABLED         | 默认为True表示开启cookie传递功能，即每次请求带上前一次的cookie，做状态保持 |
| COOKIES_DEBUG           | 默认为False表示日志中不显示cookie的传递过程              |
| LOG_LEVEL               | 默认为DEBUG，控制日志的等级                         |
| LOG_FILE                | 设置log日志文件的保存路径，如果设置该参数，日志信息将写入文件，终端将不再显示，且受到LOG_LEVEL日志等级的限制 |

**scrapy_redis的配置**

| 配置                | 说明           |
| ----------------- | ------------ |
| DUPEFILTER_CLASS  | 指纹生成以及去重类    |
| SCHEDULER         | 调度器类         |
| SCHEDULER_PERSIST | 持久化请求队列和指纹集合 |
| ITEM_PIPELINES    | 数据存入redis的管道 |
| REDIS_URL         | redis的url    |

**scrapy_splash配置**

| 配置                     | 说明              |
| ---------------------- | --------------- |
| SPLASH_URL             | 渲染服务的url        |
| DOWNLOADER_MIDDLEWARES | 下载器中间件          |
| DUPEFILTER_CLASS       | 指纹生成以及去重类       |
| HTTPCACHE_STORAGE      | 使用Splash的Http缓存 |

### 三、scrapy的其他配置

**scrapy_redis和scrapy_splash配合使用**

- scrapy-redis中配置了”DUPEFILTER_CLASS” : “scrapy_redis.dupefilter.RFPDupeFilter”，与scrapy-splash配置的DUPEFILTER_CLASS = ‘scrapy_splash.SplashAwareDupeFilter’ 相冲突！
- 查看了scrapy_splash.SplashAwareDupeFilter源码后，发现他继承了scrapy.dupefilter.RFPDupeFilter，并重写了request_fingerprint()方法。
- 比较scrapy.dupefilter.RFPDupeFilter和scrapy_redis.dupefilter.RFPDupeFilter中的request_fingerprint()方法后，发现是一样的，因此重写了一个SplashAwareDupeFilter，继承scrapy_redis.dupefilter.RFPDupeFilter，其他代码不变。

**重写dupefilter去重类**

```python
from __future__ import absolute_import

from copy import deepcopy

from scrapy.utils.request import request_fingerprint
from scrapy.utils.url import canonicalize_url

from scrapy_splash.utils import dict_hash

from scrapy_redis.dupefilter import RFPDupeFilter


def splash_request_fingerprint(request, include_headers=None):
    """ Request fingerprint which takes 'splash' meta key into account """

    fp = request_fingerprint(request, include_headers=include_headers)
    if 'splash' not in request.meta:
        return fp

    splash_options = deepcopy(request.meta['splash'])
    args = splash_options.setdefault('args', {})

    if 'url' in args:
        args['url'] = canonicalize_url(args['url'], keep_fragments=True)

    return dict_hash(splash_options, fp)


class SplashAwareDupeFilter(RFPDupeFilter):
    """
    DupeFilter that takes 'splash' meta key in account.
    It should be used with SplashMiddleware.
    """
    def request_fingerprint(self, request):
        return splash_request_fingerprint(request)
```

**scrapy_redis和scrapy_splash配合使用的配置**

```python
# 渲染服务的url
SPLASH_URL = 'http://127.0.0.1:8050'
# 下载器中间件
DOWNLOADER_MIDDLEWARES = {
    'scrapy_splash.SplashCookiesMiddleware': 723,
    'scrapy_splash.SplashMiddleware': 725,
    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
}
# 使用Splash的Http缓存
HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'

# 去重过滤器
# DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'
# DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter" # 指纹生成以及去重类
DUPEFILTER_CLASS = 'test_splash.spiders.splash_and_redis.SplashAwareDupeFilter' # 混合去重类的位置

SCHEDULER = "scrapy_redis.scheduler.Scheduler" # 调度器类
SCHEDULER_PERSIST = True # 持久化请求队列和指纹集合, scrapy_redis和scrapy_splash混用使用splash的DupeFilter!
ITEM_PIPELINES = {'scrapy_redis.pipelines.RedisPipeline': 400} # 数据存入redis的管道
REDIS_URL = "redis://127.0.0.1:6379" # redis的url
```

- CONCURRENT_REQUESTS 设置并发请求的数量，默认是16个
- DOWNLOAD_DELAY 下载延迟，默认无延迟，单位为秒
- 其他设置参考：[https://www.jianshu.com/p/df9c0d1e9087](https://www.jianshu.com/p/df9c0d1e9087)

### 四、scrapyd的使用

**scrapyd的介绍**

scrapyd是一个用于部署和运行scrapy爬虫的程序，它允许你通过JSON API来**部署爬虫项目和控制爬虫运行**，scrapyd是一个守护进程，监听爬虫的运行和请求，然后启动进程来执行它们

> 所谓json api本质就是post请求的webapi

**安装**

scrapyd服务: `pip install scrapyd`

scrapyd客户端: `pip install scrapyd-client`

**启动scrapy服务**

1. **在scrapy项目路径下** 启动scrapyd的命令：`sudo scrapyd` 或 `scrapyd`
2. 启动之后就可以打开本地运行的scrapyd，浏览器中访问本地6800端口可以查看scrapyd的监控界面`127.0.0.1:6800`

### 五、scrapy项目部署

编辑需要部署的项目的scrapy.cfg文件(需要将哪一个爬虫部署到scrapyd中，就配置该项目的该文件)

```
 [deploy:部署名(部署名可以自行定义)]
 url = http://localhost:6800/
 project = 项目名(创建爬虫项目时使用的名称)
```

**部署项目到scrapyd**

```
scrapyd-deploy -p 项目名称			# 另外打开一个窗口，进入虚拟环境会生蛋
```

**管理scrapy项目**

- 启动项目：`curl http://localhost:6800/schedule.json -d project=project_name -d spider=spider_name`
- 关闭爬虫：`curl http://localhost:6800/cancel.json -d project=project_name -d job=jobid`

**使用requests模块控制scrapy项目**

```python
# 启动爬虫
url = 'http://localhost:6800/schedule.json'
data = {
    'project': 项目名,
    'spider': 爬虫名,
}
resp = requests.post(url, data=data)

# 停止爬虫
url = 'http://localhost:6800/cancel.json'
data = {
    'project': 项目名,
    'job': 启动爬虫时返回的jobid,
}
resp = requests.post(url, data=data)
```

> 百度搜索Gerapy，了解多爬虫管理工具

### 五、crontab和Python的配合使用

**crontab在爬虫中的使用**

1. 把爬虫启动命令写入sh文件
2. 给sh脚本添加可执行权限
3. 编辑crontab的配置文件

**以spider,sh为例**

**先把要执行的命令写入脚本**

```
cd `dirname $0` || exit 1
/home/python/.virtualenvs/Spider/bin/python ./main.py >> run.log 2>&1
```

其中>>表示重定向，把print等信息导入log中

```
cd `dirname $0` || exit 1
```

**添加可执行权限**

```
sudo chmod +x myspder.sh

```

**写入crontab中**

```
crontab -e

```

**进入编辑页面并添加:**

```
 0 6 * * * /home/ubuntu/..../myspider.sh >> /home/ubuntu/.../run.log 2>&1
```

sh脚本文件也可能会报错，对应的可以把其输出和错误重定向到run2.log中