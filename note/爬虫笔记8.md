### 一、pymongo创建连接对象

**选择操作的集合**

```python
from pymongo import MongoClient
client = MongoClient(host,port)
collection = client[db名][集合名]
```

**对于权限用户**

```python
user = 'turing'
password = 'turing'

host = '127.0.0.1'
port = 27017

uri = 'mongodb://{}:{}@{}'.format(user, password, host)

client = MongoClient(uri, port=port)
```



### 二、pymongo增删改查 
**添加数据**

insert可以批量的插入数据列表，也可以插入一条数据

```
collection.insert([{"name":"test10010","age":33},{"name":"test10011","age":34}]/{"name":"test10010","age":33})
```

**添加一条数据**

```
ret = collection.insert_one({"name":"test10010","age":33})
print(ret)
```

**添加多条**

```python
item_list = [{"name":"test1000{}".format(i)} for i in range(10)]
#insert_many接收一个列表，列表中为所有需要插入的字典
t = collection.insert_many(item_list)
```

**查找一条**

```python
#find_one查找并且返回一个结果,接收一个字典形式的条件
t = collection.find_one({"name":"test10005"})
print(t)
```

**查找全部**

> 结果是一个Cursor游标对象，是一个可迭代对象，可以类似读文件的指针，但是只能够进行一次读取

```python
t = collection.find({"name":"TuringEmmy"})

for i in t:
    print(i)
for i in t: #此时t中没有内容
    print(i)
```

**更新一条**

```python
ret = col.update({'turing': 0}, {'$set': {'name': 'emmy'}}, multi=False, upsert=True)
```

**更新多条**

```python
rets = col.update({}, {'$set': {'name_id': "long"}}, multi=True, upsert=True)
```

**删除一条**

```python
ret = col.delete_one({"id": 0})
```

**删除多条**

```python
col.delete_many({'name_id': 'long'})
col.delete_many({})
```



### 三、scrapy的352

> Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架

| 异步   | 调用在发出之后，这个调用就直接返回，不管有无结果；异步是过程           |
| ---- | ---------------------------------------- |
| 非阻塞  | 关注的是程序在等待调用结果（消息，返回值）时的状态，指在不能立刻得到结果之前，该调用不会阻塞当前线程。 |

参考文档:[http://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/overview.html](http://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/overview.html)

### 四、scrapy的工作流程

![爬虫流程-3](image/爬虫流程-3.png)

1. 调度器把requests-->引擎-->下载中间件--->下载器
2. 下载器发送请求，获取响应---->下载中间件---->引擎--->爬虫中间件--->爬虫
3. 爬虫提取url地址，组装成request对象---->爬虫中间件--->引擎--->调度器
4. 爬虫提取数据--->引擎--->管道
5. 管道进行数据的处理和保存

### 五、scrapy的使用

**创建scrapy项目**

`scrapy startproject spider_name`

**创建爬虫**

命令：**在项目路径下执行**:scrapy genspider +<爬虫名字> + <允许爬取的域名>

```python
cd spider_name
scrapy genspider baidu baidu.com
```

**完善代码**

对`spider_name.py`文件进行编写

```python
# -*- coding: utf-8 -*-
import scrapy

class TuringSpider(scrapy.Spider):
    # 爬虫名
    name = 'turing'
    # 爬去范围域名，可以是多个
    allowed_domains = ['baidu.com']
    # 可以是多个，元组或者列表
    start_urls = ['http://www.baidu.com']

    def parse(self, response):
        print(response.url)
        print(response.request.url)
        print(response.headers)
        print(response.request.headers)
        print(response.status)
```

> 1. response.xpath方法的返回结果是一个类似list的类型，其中包含的是selector对象，操作和列表一样，但是有一些额外的方法
> 2. extract() 返回一个包含有字符串的列表
> 3. extract_first() 返回列表中的第一个字符串，列表为空没有返回None
> 4. spider中的parse方法必须有
> 5. 需要抓取的url地址必须属于allowed_domains,但是start_urls中的url地址没有这个限制
> 6. 启动爬虫的时候注意启动的位置，是在项目路径下启动

**管道处理数据**

在爬虫文件baidu.py中parse()函数中最后添加

```
yield item
```

思考：为什么要使用yield?

1. 让整个函数变成一个生成器
2. 遍历这个函数的返回值的时候，挨个把数据读到内存买不会造成内存的瞬间占用过高

**运行scrapy**

```python
scrapy crawl baidu
```

### 六、scrapy的常用属性

| 属性                       | 说明          |
| ------------------------ | ----------- |
| response.url             | 响应的url      |
| response.request.url     | 响应对应的请求的url |
| response.headers         | 响应的头        |
| response.request.headers | 响应对应的请求的头   |
| response.status          | 响应的状态码      |
| response.body            | 响应的内容 bytes |

