### 一、构造request的腾讯招聘的爬虫

> 通过爬取腾讯招聘的页面的招聘信息,学习如何实现翻页请求
>
> 地址：[http://hr.tencent.com/position.php](http://hr.tencent.com/position.php)

**思路分析**

- 获取首页的数据
- 寻找下一页的地址，进行翻页，获取数据

> 设置ROBOTS协议和User-Agent

**scrapy.Request的更多参数**

```
scrapy.Request(url[,callback,method="GET",headers,body,cookies,\
meta,dont_filter=False])
```

| 参数          | 说明                                       |
| ----------- | ---------------------------------------- |
| callback    | 当前的url的响应交给哪个函数去处理                       |
| meta        | 实现数据在不同的解析函数中传递，meta默认带有部分数据，比如下载延迟，请求深度等 |
| dont_filter | 默认为False，会过滤请求的url地址，即请求过的url地址不会继续被请求，对需要重复请求的url地址可以把它设置为Ture，比如贴吧的翻页请求，页面的数据总是在变化;start_urls中的地址会被反复请求，否则程序不会启动 |
| method      | 指定POST或GET请求                             |
| headers     | 接收一个字典，其中不包括cookies                      |
| cookies     | 接收一个字典，专门放置cookies                       |
| body        | 接收一个字典，为POST的数据                          |

### 二、meta的参数的使用

> meta其实是一个字典
>
> 作用：meta可以实现数据在不同的解析函数中的传递

```python
def parse(self,response):
    ...
    yield scrapy.Request(detail_url, callback=self.parse_detail,meta={"item":item})
...

def parse_detail(self,response):
    #获取之前传入的item
    item = resposne.meta["item"]
```

注意：meta字典中有一个固定的键`proxy`，表示代理ip，关于代理ip的使用我们将在scrapy的下载中间件的学习中进行介绍

### 三、BaseItem的使用

个人理解，相当于Django里面的serializer

**定义item**对应items.py文件

```python
class TencentItem(scrapy.Item): 
    name = scrapy.Field() # 招聘标题
```

**使用item**

> Item使用之前需要先导入并且实例化，之后的使用方法和使用字典相同

```python
from Tencent.items import TencentItem # 导入Item，注意路径
...
    def parse_detail(self, response):
        meta_dict = response.meta # 获取传入的meta

        item = TencentItem() # 实例化后可直接使用
        item['name'] = meta_dict['name']
        print(item)
```

### 四、构造reqeust的直接携带的cookies

需要重写`start_requests`方法

```python
 def start_requests(self):
        cookies_str = '---'

        # 转换为cookies_dict
        cookies_dict = {cookie.split('=')[0]: cookie.split('=')[1] for cookie in cookies_str.split('; ')}

        # 构造起始的request对象
        yield scrapy.Request(self.start_urls[0], callback=self.parse, cookies=cookies_dict)
```



### 五、request发送post的请求

```python
def parse(self, response):
        # 1. 对登录页进行解析,然后提取authenticity_token, 并春送到下一个解析函数当中去
        authenticity_token = response.xpath('//*[@name="authenticity_token"]/@value').extract_first()
        print(authenticity_token)

        # 构造post登录请求的data
        data = {
            'commit': 'Sign in',
            'utf8': '✓',
            'authenticity_token': authenticity_token,
            'login': 'TuringEmmy',
            'password': '258467Ylg2018'
        }
        # 2. 发送post请求,进行登录
        url = 'https://github.com/session'
        # yield scrapy.Request(url, method='POST', body=data, callback=self.check)
        yield scrapy.FormRequest(url, formdata=data, callback=self.check)
```



### 六、管道的深入使用

**常用的方法**

| 方法                             | 说明            |
| ------------------------------ | ------------- |
| process_item(self,item,spider) | 实现对item数据的处理  |
| open_spider(self, spider)      | 在爬虫开启的时候仅执行一次 |
| close_spider(self, spider)     | 在爬虫关闭的时候仅执行一次 |

**管道文件的修改**

```python
class GithubPipeline1(object):
    def open_spider(self, spider):
        # 爬虫只执行一次
        self.f = open('title.txt', 'a', encoding='utf-8')

    def process_item(self, item, spider):
        # 判断指定爬虫来使用管道!
        if spider.name=='pipline':
            self.f.write(json.dumps(item, ensure_ascii=False, indent=2) + '\n')
        # 必须return item , 为了其他后执行的管道能够收到item数据!
        return item

    def close_spider(self, spider):
        self.f.close()
```

**开启管道**

```python
ITEM_PIPELINES = {
    # 权重越小,越厉害,值表示距离引擎的远近
    'GitHub.pipelines.GithubPipeline1': 300,
    'GitHub.pipelines.GithubPipeline2': 200,
}
```

**思考：pipeline在settings中能够开启多个，为什么需要开启多个？**

1. 不同的pipeline可以处理不同爬虫的数据，通过spider.name属性来区分
2. 不同的pipeline能够对一个或多个爬虫进行不同的数据处理的操作，比如一个进行数据清洗，一个进行数据的保存
3. 同一个管道类也可以处理不同爬虫的数据，通过spider.name属性来区分

