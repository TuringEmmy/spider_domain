### 一、查看scrapy_redsi的dmoz示例爬虫

`https://github.com/rolando/scrapy-redis.git`

```python
class DmozSpider(CrawlSpider):
    """Follow categories and extract links."""
    name = 'dmoz'
    allowed_domains = ['dmoztools.net']
    start_urls = ['http://dmoztools.net/']

    # 定义数据提取规则，使用了css选择器
    rules = [
        Rule(LinkExtractor(
            restrict_css=('.top-cat', '.sub-cat', '.cat-item')
        ), callback='parse_directory', follow=True),
    ]

    def parse_directory(self, response):
        for div in response.css('.title-and-desc'):
            yield {
                'name': div.css('.site-title::text').extract_first(),
                'description': div.css('.site-descr::text').extract_first().strip(),
                'link': div.css('a::attr(href)').extract_first(),
            }
```

但是在settings.py中多了一下几行,这几行表示`scrapy_redis`中重新实现的了去重的类，以及调度器，并且使用的`RedisPipeline`

```python
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
SCHEDULER_PERSIST = True

ITEM_PIPELINES = {
    'example.pipelines.ExamplePipeline': 300,
    'scrapy_redis.pipelines.RedisPipeline': 400,
}
REDIS_URL = "redis://127.0.0.1:6379"
 #或者使用下面的方式
 # REDIS_HOST = "127.0.0.1"
 # REDIS_PORT = 6379
```

继续执行程序，会发现程序在前一次的基础之上继续往后执行，**所以domz爬虫是一个基于url地址的增量式的爬虫**

### 二、scrapy_redis原理分析

settings.py中的三个配置来进行分析 分别是：

- RedisPipeline
- RFPDupeFilter
- Scheduler

**Scrapy_redis之RedisPipeline**

RedisPipeline中观察process_item，进行数据的保存，存入了redis中

 **Scrapy_redis之RFPDupeFilter**

RFPDupeFilter 实现了对request对象的加密

 **Scrapy_redis之Scheduler**

scrapy_redis调度器的实现了决定什么时候把request对象加入带抓取的队列，同时把请求过的request对象过滤掉

由此可以总结出request对象入队的条件

> request之前没有见过
> request的dont_filter为True，即不过滤
> start_urls中的url地址会入队，因为他们默认是不过滤


### 三、指纹生成的方法和请求入队的条件

```python
added = self.server.sadd(self.key, fp)
		# fp放入集合key中，成功返回1，失败返回0

	# 指纹生成的方法	
	    fp = hashlib.sha1()
	    fp.update(to_bytes(request.method))
	    fp.update(to_bytes(canonicalize_url(request.url)))
	    	xxx.cn/s?b=2&a=1
	    	xxx.cn/s?a=1&b=2
	    	# w3lib.url.canonicalize_url函数能够把url字符串重新按照固定规律进行排序
	    fp.update(request.body or b'')
	    	{b:1, a:2}
	    	{a:2, b:1}
	    指纹 = fp.hexdigest()

	# 返回bool表达式
		return added == 0
```



### 四、单击断点续爬的实现

**RedisSpider**

1. 继承自父类为RedisSpider
2. 增加了一个redis_key的键，没有start_urls，因为分布式中，如果每台电脑都请求一次start_url就会重复
3. 多了`__init__`方法，该方法不是必须的，可以手动指定allow_domains
4. 启动方法：
   1. 在每个节点正确的目录下执行`scrapy crawl 爬虫名`，使该节点的scrapy_redis爬虫程序就位
   2. 在共用的redis中 `lpush redis_key 'start_url'`，使全部节点真正的开始运行

> 没有rules参数

```python
class MySpider(RedisSpider):
    """Spider that reads urls from redis queue (myspider:start_urls)."""
    name = 'myspider_redis'
    redis_key = 'myspider:start_urls'

    def __init__(self, *args, **kwargs):
        # Dynamically define the allowed domains list.
        domain = kwargs.pop('domain', '')
        self.allowed_domains = filter(None, domain.split(','))
        super(MySpider, self).__init__(*args, **kwargs)

    def parse(self, response):
        return {
            'name': response.css('title::text').extract_first(),
            'url': response.url,
        }
```



**RedisCrawlSpider**

1. 继承自父类为RedisCrwalSpider
2. 也是增加了一个redis_key的键，没有start_urls
3. 一样多了init方法，该方法不是必须的，可以手动指定allow_domains
4. 启动方法：
   1. 在每个节点正确的目录下执行`scrapy crawl 爬虫名`，使该节点的scrapy_redis爬虫程序就位
   2. 在共用的redis中 `lpush redis_key 'start_url'`，使全部节点真正的开始运行

> 含有rules参数

```
class MyCrawler(RedisCrawlSpider):
    """Spider that reads urls from redis queue (myspider:start_urls)."""
    name = 'mycrawler_redis'
    redis_key = 'mycrawler:start_urls'

    # allowed_domains=[]

    rules = (
        # follow all links
        Rule(LinkExtractor(), callback='parse_page', follow=True),
    )

    def __init__(self, *args, **kwargs):
        # Dynamically define the allowed domains list.
        domain = kwargs.pop('domain', '')
        self.allowed_domains = filter(None, domain.split(','))
        super(MyCrawler, self).__init__(*args, **kwargs)

    def parse_page(self, response):
        return {
            'name': response.css('title::text').extract_first(),
            'url': response.url,
        }
```



### 五、scrapy_redis实现分布式

```python
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
SCHEDULER_PERSIST = True
ITEM_PIPELINES = {'scrapy_redis.pipelines.RedisPipeline': 400,}
REDIS_URL = "redis://127.0.0.1:6379"
```

1. 在爬虫文件中爬虫继承RedisSpider类，或RedisCrawlSpider类
2. 在爬虫文件中redis_key替代了start_urls
3. 手动指定allowed*domain来代替_init*函数
4. RedisCrawlSpider类完善rules规则，并且不能使用parse()方法
5. 不管是不是分布式爬虫，都要正确配置REDIS_URL
6. 通过`scrapy crawl spider`启动爬虫后，向redis_key放入一个或多个起始url（lpush或rpush都可以），才能启动scrapy_redis爬虫
7. 除了以上差异点以外，scrapy_redis爬虫和scrapy爬虫的使用方法都是一样的

### 六、scrapy_splash组件

> scrapy-splash能够模拟浏览器加载js，并返回js运行后的数据

**安装并启动docker服务**

安装参考 [https://blog.csdn.net/sanpic/article/details/81984683](https://blog.csdn.net/sanpic/article/details/81984683)

**获取splash的镜像**

> 在正确安装docker的基础上pull取splash的镜像

`sudo docker pull scrapinghub/splash`

**验证是否安装成功**

> 运行splash的docker服务，并通过浏览器访问8050端口验证安装是否成功

- 前台运行 `sudo docker run -p 8050:8050 scrapinghub/splash`
- 后台运行 `sudo docker run -d -p 8050:8050 scrapinghub/splash`

访问 [http://127.0.0.1:8050](http://127.0.0.1:8050) 观察是否成功

**解决镜像源超时的问题**

1. 创建并编辑docker的配置文件

`sudo vi /etc/docker/daemon.js`

2. 写入国内docker-cn.com的镜像地址配置后保存退出

```
{ 
"registry-mirrors": ["https://registry.docker-cn.com"] 
}
```

重启电脑或docker服务后重新获取splash镜像

**关闭splash服务**

> 需要先关闭容器后，再删除容器

```
sudo docker ps -a
sudo docker stop CONTAINER_ID
sudo docker rm CONTAINER_ID
```

### 七、在scrapy中使用splash

**创建爬虫项目**

```
scrapy startproject test_splash
cd test_splash
scrapy genspider no_splash baidu.com
scrapy genspider with_splash baidu.com
```

**setting的配置**

```python
# 渲染服务的url
SPLASH_URL = 'http://127.0.0.1:8050'
# 下载器中间件
DOWNLOADER_MIDDLEWARES = {
    'scrapy_splash.SplashCookiesMiddleware': 723,
    'scrapy_splash.SplashMiddleware': 725,
    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
}
# 去重过滤器
DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'
# 使用Splash的Http缓存
HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'

# Obey robots.txt rules
ROBOTSTXT_OBEY = False
```

**不使用splash**

```
class NoSplashSpider(scrapy.Spider):
    name = 'no_splash'
    allowed_domains = ['baidu.com']
    start_urls = ['https://www.baidu.com/s?wd=13161933309']

    def parse(self, response):
        with open('no_splash.html', 'w') as f:
            f.write(response.body.decode())
```

**使用splash**

```python
class WithSplashSpider(scrapy.Spider):
    name = 'with_splash'
    allowed_domains = ['baidu.com']
    start_urls = ['https://www.baidu.com/s?wd=13161933309']

    def start_requests(self):
        yield SplashRequest(self.start_urls[0],
                            callback=self.parse_splash,
                            args={'wait': 10}, # 最大超时时间，单位：秒
                            endpoint='render.html') # 使用splash服务的固定参数

    def parse_splash(self, response):
        with open('with_splash.html', 'w') as f:
            f.write(response.body.decode())
```

**分别运行两个爬虫**

```
scrapy crawl no_splash
scrapy crawl with_splash
```

> 1. splash类似selenium，能够像浏览器一样访问请求对象中的url地址
> 2. 能够按照该url对应的响应内容依次发送请求
> 3. 并将多次请求对应的多次响应内容进行渲染
> 4. 最终返回渲染后的response响应对象

**更多**

> 关于splash [https://www.cnblogs.com/zhangxinqi/p/9279014.html](https://www.cnblogs.com/zhangxinqi/p/9279014.html)
>
> 关于scrapy_splash（截屏，get_cookies等） [https://www.e-learn.cn/content/qita/800748](https://www.e-learn.cn/content/qita/800748)

