1. middlewares.py中间件
```
 # 根据逻辑位置不同分为
   	# 爬虫中间件
   	# 下载中间件
   # 对request、response进行预处理
   	# 替换UA
   	# 使用代理ip
   	# 检查响应
   	# 替换cookie
   # 中间件中的方法
   	process_request(request, spider)
   		# 不写return 
   		# 当request经过中间件时被调用
   	process_response(request, response, spider)
   		# return response
   		# 当response经过中间件时被调用
   	# 俩个方法固定的返回套路
   		# return request -->引擎-->调度器入队
   		# return response -->(引擎)-->爬虫解析
   # 要在settings.py中设置开启中间件
   	# 跟管道配置一样，左边是位置，右边是权重值
   	# 权重值越小，越优先执行
```

2. scrapy和selenium配合使用
```
 # 利用selenium调用浏览器自动的处理js或发送请求，获取cookies
   # scrapy代码中调用封装好的selenium的函数获取cookies再使用
   # scrapy框架能够完整的展示selenium的日志信息
```

3. crawlspider爬虫类
```
class CrawlSpider(scrapy.spiders.CrawlSpider):
   	name = 爬虫名
   	allowed_domains = [范围域名]
   	start_urls = [起始url]
   	# 不能写名为parse的解析函数，该函数用来实现rules规则集合的功能
   	# 多了一个rules规则集合
   	rules = (
   		Rule(LinkExtractor(规则参数)),
   			# 按照连接提取器中的规则参数提取url，构造request；框架会自动发送请求获取响应
   	
   		Rule(LinkExtractor(规则参数),
   			 follow=True),
   			 # 按照连接提取器中的规则参数提取url，构造request；框架会自动发送请求获取响应
   			 # 该响应还会进入rules规则集合中被Rule规则对象所处理
   	
   		Rule(LinkExtractor(规则参数),
   			 callback='解析函数名字符串'),
   			 # 按照连接提取器中的规则参数提取url，构造request；框架会自动发送请求获取响应
   			 # 该响应会进入callback回调解析函数中进行数据提取
   	
   		Rule(LinkExtractor(规则参数),
   			 callback='解析函数名字符串', 
   			 follow=True),
   			 # 按照连接提取器中的规则参数提取url，构造request；框架会自动发送请求获取响应
   			 # 该响应还会进入rules规则集合中被Rule规则对象所处理
   			 # 该响应会进入callback回调解析函数中进行数据提取
   	
   		# 关于LinkExtractor链接提取器
   			# 是Rule对象中的必要参数
   			# 参数：
   				# allow:按照 正则匹配a标签中href属性的值 提取url
   				# restrict_xpaths:xpath规则定位标签范围，在定位的标签范围内的url都会被提取
   				# 多个参数共同使用，被提取的url一定满足所有的规则参数！
   	)

   # Rule规则对象中指定的callback回调函数之间不能传递数据
   # 除此以外，crawlspider的解析函数中用法和scrapy.Spider爬虫用法一致！
   # 创建crawlspider爬虫文件的命令，在项目路径下执行 
   	# scrapy genspider -t crawl 爬虫名 爬取范围域名
```

4. scrapy_redis
```
# scrapy_redis是scrapy框架的分布式组件
   # scrapy_redis的工作流程是在scrapy工作流程基础上，利用共用的redis实现了请求队列和请求指纹集合
   # scrapy_redis作用：断点续爬、分布式快速爬取
   	# 利用共用的redis实现了请求队列和请求指纹集合
```