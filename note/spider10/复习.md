1. scrapy构造request对象
```
scrapy.Request(url,
   			   callback=,
   			   headers={},
   			   cookies={},
   			   method='GET',
   			   body="{json_str}",
   			   meta={},
   			   dont_filter=False)
   scrapy.FormRequest(url,
   				   # 没有method、body
   				   formdata={post_data}
   				   # 其他参数都一样)
```

2. 关于BaseItem
```
在items.py中去定义完善BaseItem类
   	字段 = scrapy.Field()
   在spider.py中去导入BaseItem类
   实例化后，像字典一样使用
   	item = BaseItem类()
   	item['字段'] = ...
   	yield item # BaseItem {} Request None
```

3. 关于管道
```
 在pipelines.py中去完成管道类
   	process_item(item, spider)
   		return item
   		# 当返回一个数据item时，就会被调用
   		# 参数spider利用name属性去区别处理
   	open_spider(spider) # 爬虫开启时仅执行一次
   	close_spider(spider) # 爬虫关闭时仅执行一次
   在settings.py中开启管道的配置
   	# 左边是位置，右边是权重
   	# 权重值越小，越优先执行
   多爬虫可以共用一个管道
   一个爬虫可以使用多个管道
```

4. 对于起始url需要登陆后才能获取的，需要重写scrapy.Spider.start_requests函数
```
 def start_requests(self):
   	# 对start_url构造request，并返回
   	for url in self.start_urls:
   		yield scrapy.Request(url, 
   							 callback=self.parse,
   							 cookies=cookies_dict,
   							 dont_filter=True)
```