1. scrapy中间件的使用
```
 预处理request和response对象

   process_request(self, request, spider)
   	不写return! # 正常传递request
   	#return Response # 不再发请求，直接给引擎
   	#return Request # 该对象直接返回引擎--调度器入队

   process_response(self, request, response, spider)
   	return response
   	# request.dont_filter = True
   	# return request # 直接返回引擎--调度器入队

   像管道类一样，需要在settings.py中设置开启
   像管道类一样，左边是位置，右边是权重
   权重值越小，越优先执行！
```

2. 代理ip的使用
```
 	proxy = 'https://1.71.188.37:3128' 
   	 request.meta['proxy'] = proxy
```

3. scrapy和selenium配合使用
```
 # 利用selenium做模拟登陆后获取cookies
   # 把利用selenium获取cookies封装为一个方法，在scrapy中调用
   # scrapy能够完整的记录selenium运行的日志信息！
```

4. crawlspider爬虫
```
scrapy genspider -t crawl 爬虫名 范围域名
   # 用很少的代码快速获取url地址，获取对应响应并处理
   # rules规则元祖中的Rule规则对象中callback回调函数互相之间不能传递数据
   # 在解析函数中可以正常去构造request并返回，和scrapy.Spider爬虫类在解析函数中的用法是一样的
```

5. scrapy_redis的作用
   通过持久化请求队列和请求的指纹集合来实现：
   	断点续爬
   	分布式快速抓取

6. scrapy_redis的工作流程
   在scrapy工作流程基础上，利用共用的redis实现请求队列和指纹集合，存放request对象；获取的数据默认也放到共用的redis中