1. 中间件middlewares.py
```
	根据逻辑位置不同：
		下载中间件
		爬虫中间件
	对request、response预处理
		替换User-Agent
		替换Cookies
		使用代理ip
		对响应内容做检查
	process_request(request, spider)
		当request经过中间件时，该函数被调用
		# 不要return：让request能够正常的流转
	process_response(request, response, spider)
		当response经过中间件时，该函数被调用
		# return response
	# return request --> 引擎 --> 调度器 --> 入队
	# return response --> ... --> 爬虫 --> 解析

	在settings.py中开启中间件
		左边是位置 右边是权重值
		权重值越小 越优先执行


```

2. crawlspider爬虫类
```
	# scrapy genspider -t crawl 爬虫名 范围域名
	# 根据跟则自动提取url，自动按照callback以及follow参数去执行
	# rules中的callback指定的解析函数之间不能传递数据
	# 除此以外跟scrapy.Spider爬虫类一样

	class CrawlSpider(scrapy.spiders.CrawlSpider):
		name = 
		allowed_domains = []
		start_urls = []
		# 不能写名为parse的函数，该函数用来实现了rules规则元祖的部分功能
		rules = (
			Rule(LinkExtractor(规则参数))
				# 按照规则参数对响应进行提取url，构造成request，框架会自动发送请求，获取响应

			Rule(LinkExtractor(规则参数),
				 callback='func')	
				# 按照规则参数对响应进行提取url，构造成request，框架会自动发送请求，获取响应
				# 该响应会进入callback指定的解析函数中处理

			Rule(LinkExtractor(规则参数),
				 follow=True)
				# 按照规则参数对响应进行提取url，构造成request，框架会自动发送请求，获取响应
				# 该响应会进入rules规则元祖中被提取处理

			Rule(LinkExtractor(规则参数),
				 callback='func',
				 follow=True)				
				# 按照规则参数对响应进行提取url，构造成request，框架会自动发送请求，获取响应
				# 该响应会进入callback指定的解析函数中处理
				# 同时，该响应也会进入rules规则元祖中被提取处理
			# 链接提取器LinkExtractor(规则参数)的参数
				# allow：re匹配a标签href属性的值
				# restrict_xpaths：xpath定位某个标签，该标签范围内的url都会被提取
				# deny allow_domains deny_domains
				# 最终结果一定符合所有参数规则
		)
```

3. scrapy_redis
```
 scrapy_redis作用
   	断点续爬
   	分布式
 scrapy_redis工作流程
   	在scrapy工作流程的基础上，利用共用的redis实现了request队列和request指纹集合，并保存了数据
```

4. request对象进入请求队列的条件
```
request.dont_filter == True
request.指纹 not in 指纹集合 
```