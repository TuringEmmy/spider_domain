1. scrapy_redis能够实现断点续爬和分布式

2. scrapy_redis让每个进程共用同一个redis，在其中实现了request队列和request的指纹集合

3. request指纹的生成
```
	1. hashlib.sha1()
	2.  request.url # 需要排序
		request.method
		request.post_data or '' # 也需要排序
```

4. request入队的条件
```
1. 请求对象不做过滤去重
      request.dont_filter == True
2. 请求对象的指纹不在指纹集合中
      request.fp not in fp_set
```

5. scrapy_redis的settings配置
```
	# 指定指纹去重类
		DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
	# 指定调度器类
		SCHEDULER = "scrapy_redis.scheduler.Scheduler"
	# 持久化请求队列和请求指纹集合
		SCHEDULER_PERSIST = True 
	# 使用默认的scrapy_redis数据管道，把数据保存到redis
		ITEM_PIPELINES = {
		    'scrapy_redis.pipelines.RedisPipeline': 400
		}
	# 指定使用哪个redis
		REDIS_URL = 'redis://127.0.0.1:6379'

```



6. scrapy_redis爬虫类

```
	scrapy.Spider
	scrapy.spiders.CrawlSpider
		# 以上俩个单机可以实现断点续爬
		# 一旦请求队列暂时为空，提前结束
		# 而scrapy_redis的俩个爬虫类则不能自动结束

	scrapy_redis.spiders.RedisSpider
	scrapy_redis.spiders.RedisCrawlSpider
		# start_urls被redis_key替代，写法上没有区别
		# 启动方法改变：
			# scrapy crawl 爬虫名
			# 向共用的redis的指定的redis_key中lpush/rpush起始的url
```

7. scrapy_splash组件
```
	利用该组件可以获取渲染后的响应对象

	docker pull scrapinghub/splash
	sudo docker run -p 8050:8050 scrapinghub/splash (-d)
	代码中使用scrapy_splash.SplashRequest来构造请求对象
	在settings.py中开启scrapy_splash的配置
		# 渲染服务的url
		SPLASH_URL = 'http://127.0.0.1:8050'
		# 下载器中间件
		DOWNLOADER_MIDDLEWARES = {
		    'scrapy_splash.SplashCookiesMiddleware': 723,
		    'scrapy_splash.SplashMiddleware': 725,
		    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
		}
		# 去重过滤器
		DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'
		# 使用Splash的Http缓存
		HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'
```