1. scrapy_redis的关键配置

```
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter" # 将request转换为指纹

SCHEDULER = "scrapy_redis.scheduler.Scheduler" # request调度器

SCHEDULER_PERSIST = True # 表示是否持久化request队列和request指纹
# SCHEDULER_PERSIST为True 程序结束时不删除request的队列和指纹集合
# SCHEDULER_PERSIST为False 程序结束时会删除request的队列和指纹集合

ITEM_PIPELINES = {
    'example.pipelines.ExamplePipeline': 300,
    'scrapy_redis.pipelines.RedisPipeline': 400, # 把Item存入redis
}

REDIS_URL = 'redis://127.0.0.1:6379
```


2. request指纹由 url method data


3. scrapy_redis.pipelines.RedisPipeline
```
	# 关键代码：
		redis链接操作对象.rpush(key, 序列化成字符串的item数据)
	# 拼接字符串的方法
	'%(spider)s:items' % {'spider': spider.name}
	'%s:items' % spider.name
	'%s:items' % 'dmoz'
	key = 'dmoz:items'
```


4. scrapy_redis.dupefilter.RFPDupeFilter
```
added = self.server.sadd(self.key, fp)
		# fp放入集合key中，成功返回1，失败返回0

	# 指纹生成的方法	
	    fp = hashlib.sha1()
	    fp.update(to_bytes(request.method))
	    fp.update(to_bytes(canonicalize_url(request.url)))
	    	xxx.cn/s?b=2&a=1
	    	xxx.cn/s?a=1&b=2
	    	# w3lib.url.canonicalize_url函数能够把url字符串重新按照固定规律进行排序
	    fp.update(request.body or b'')
	    	{b:1, a:2}
	    	{a:2, b:1}
	    指纹 = fp.hexdigest()
```
5. scrapy_redis.scheduler.Schedule
```
	if not request.dont_filter and 
		# 过滤去重并且指纹在集合中
	   self.df.request_seen(request):

	request请求对象进入队列的条件：
		a. request.dont_fitler == True 不过滤所以一定会进入队列
		b. request.dont_filter == False and request.fp not in fp_set
		过滤并且指纹不在集合中，也会进入队列

### 关于request入队的条件的分析
	def A():
	    if not request.dont_filter and self.df.request_seen(request):
	        return xxxx
    # if True(要做去重) and Ture(fp在集合中) --> return
    # if True(要做去重) and False(fp不在集合中) --> 不return , request入队
    # if False(不做去重) and xxxx --> 不return , request入队
    queue.push(request)   
	"""
	request.dont_filter 默认= False # 表示要做去重
	self.df.request_seen(request) 返回 true或者false
	    # fp在集合中返回true
	    # 不在集合中返回false
	"""
	# 请求进入队列的条件
	    # 1. request.dont_filter == True
	    # 2. request.fp not in fp_set
```



6. 增量式爬虫
```
基于url地址的增量式爬虫
	url地址变，内容也跟着变化
基于内容的增量式爬虫
    url地址不变，内容发生变化
```


7. 利用scrapy_redis实现单机断点续爬
```
 DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter" # 指定生成指纹的scrapy_redis类
	SCHEDULER = "scrapy_redis.scheduler.Scheduler" # 指定使用scrapy_redis的调度器
	SCHEDULER_PERSIST = True # 持久化队列和集合
	ITEM_PIPELINES = {
	    'scrapy_redis.pipelines.RedisPipeline': 400, # 指定使用scrapy_redis数据管道
	}
	REDIS_URL = 'redis://127.0.0.1:6379' # 指定使用的redis

```

8. 分布式爬虫
```
	 1. 继承类发生改变
		scrapy_redis.spiders.RedisSpider
			# 必须有名为parse的函数
			# 跟scrapy.Spider爬虫类一样
		scrapy_redis.spisers.RedisCrawlSpider
			# 不能写名为parse的函数
			# 跟scrapy.spiders.CrawlSpider爬虫类一样
	2. start_urls被redis_key所替代
		启动分布式爬虫的方法发生改变：
			# scrapy crawl 爬虫名 让爬虫就绪
			# 向redis_key中rpush/lpush起始的url
	3. 直接声明allowed_domains爬取范围就可以了，无需重写__init函数
	4. settings.py中多了scrapy_redis的配置！
```

9. 分布式爬虫代码实现
```
	1. 完成单机的爬虫代码
   2. 添加scrapy_redis的配置
   3. 改变继承类
   4. start_urls被redis_key替代
   5. 向要部署的节点上传代码，每个节点都让爬虫进程就绪
      scrapy crawl 爬虫名
      # 此时爬虫进程会抢起始url和抢请求队列中的请求对象
   6. 向redis_key中push起始url
```

10. scrapy_splash
```
	# scrapy_splash使用splash服务，就好像selenium
	# 获取镜像
		docker pull scrapinghub/splash
	# 运行splash服务
		sudo docker run -p 8050:8050 scrapinghub/splash (-d)
	# pip install scrapy-splash

	# 使用scrapy_splash.SplashRequest请求对象

	# settings.py中配置scrapy_splash
		# 渲染服务的url
		SPLASH_URL = 'http://127.0.0.1:8050'
		# 下载器中间件
		DOWNLOADER_MIDDLEWARES = {
		    'scrapy_splash.SplashCookiesMiddleware': 723,
		    'scrapy_splash.SplashMiddleware': 725,
		    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
		}
		# 去重过滤器
		DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'
		# 使用Splash的Http缓存
		HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'
```

