1.  增量式爬虫
    url增量：url地址变化，内容也随之变化
    内容增量：url不变，页面中数据发生变化

2.  scrapy_redis实现增量式爬虫的步骤

```
	1. 完成scrapy.Spider或scrapy.spiders.CrawlSpider爬虫，且在本地测试运行成功
	2. 在settings.py中开启scrapy_redis的配置
		a. DUPEFILTER_CLASS = 指定生成request.fp以及去重的类
		b. SCHEDULER = 指定调度器类
		c. SCHEDULER_PERSIST = True 持久化request队列和fp集合
		d. ITEM_PIPELINES = {'scrapy_redis.pipelines.RedisPipeline': 400} 指定保存数据到redis的管道类
		e. REDIS_URL = 指定redis的ip和port
	3. 更改爬虫的继承类为
		scrapy_redis.spiders.RedisSpider
		scrapy_redis.spiders.RedisCrawlSpider
	4. 用redis_key替代start_urls
	5. scrapy crawl 爬虫名
	6. 向redis_key中push起始的url
```

3.  四种爬虫类的对比
```
	scrapy.Spider
	scrapy.spiders.CrawlSpider
	scrapy_redis.spiders.RedisSpider
	scrapy_redis.spiders.RedisCrawlSpider

	scrapy爬虫类跟scrapy_redis爬虫类的区别
		scrapy爬虫有start_urls
		scrapy_redis爬虫有redis_key
		scrapy爬虫在队列为空时，自动退出
		scrapy_redis爬虫在队列为空时，还在等待获取不会退出
	(Redis)Spider爬虫跟(Redis)CrawlSpider爬虫类的区别
		(Redis)Spider爬虫有名为parse的函数
		(Redis)CrawlSpider爬虫有rules规则元祖
```

4.  pipelines.py管道
```
	process_item(item, spider)
		# 管道类必须有该函数，每当返回一个数据时，被调用
		# return item
	open_spider(spider) # 爬虫开启时仅执行一次
	close_spider(spider) # 爬虫关闭时仅执行一次
	# settings.py中开启管道：左地址右权重，值越小先执行
```

5.  middlewares.py中间件
```
	process_request(request, spider)
		# 当request经过中间件时
		# 不要写return
	process_response(request, response, spider)
		# 当response经过中间件时
		# 要return response
	# settings.py中开启中间件：左地址右权重，值越小先执行
```

6.  scrapy_splash组件
```
	通过splash获取自动加载渲染之后的响应对象
	sudo docker pull scrapinghub/splash
	sudo docker run -d -p 8050:8050 scrapinghub/splash
	在scrapy爬虫代码中使用scrapy_splash.SplashRequest类来构造请求对象
	settings.py中开启scrapy_splash的配置
		1. 指定splash服务所在的url和端口
		2. 指定使用scrapy_splash的指纹去重类
		3. 指定使用scrapy_splash的特定三个中间件
		4. 指定使用spalsh服务的http缓存
```

7.  请求对象的指纹的生成
    a. hashlib.sha1()
    b. url method post_data/''

8.  请求对象进入队列的条件
    a. request.fp not in fp_set
    b. request.dont_filter == True

9.  7、8两点和'使用共用的redis存放请求队列和指纹集合'一起，使scrapy_redis能够实现 断点续爬 和 分布式！