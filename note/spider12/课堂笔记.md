1. scrapy_redis和scrapy_splash混合使用
```
	1. 重写dupefilter指纹去重类
	2. 正常完成scrapy_splash单机版的爬虫
		# 注意：不能重写start_requests函数
	3. 更改为scrapy_redis爬虫的继承类
	4. redis_key替代start_urls
	5. 在settings.py中配置
		a. robotstxt ua default_request_headers
		
		b. 指定redis
		c. 指定scheduler
		d. 持久化scheduler_persist
		e. 指定scrapy_redis管道
		f. 指定重写的dupefilter指纹去重类！ # 这使关键！
		g. 指定三个scrapy_splash中间件
		h. 指定使用splash缓存
		i. 指定spalsh服务的url
```
2. scrapyd部署控制scrapy爬虫
```
	1. 在项目路径下 scrapyd 或 sudo scrapyd
	2. 修改scrapy项目的配置文件（scrapy.cfg）
		# 指定scrapyd服务的webapi的url地址
	3. 在项目路径下 scrapyd-deploy -p 项目名
	4. 可以通过webapi发送post请求控制爬虫的启动和停止
		# http://localhost:6800/schedule.json 
			post_data: project spider
			return: jobid
			# 相当于：scrapy crawl 爬虫名
		# http://localhost:6800/cancel.json
			post_data: project job
			return: status

```

