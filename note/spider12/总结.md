1. 可以根据日志信息去查看scrapy的默认开启的中间件的源代码
2. settings.py的配置



		ROBOTSTXT_OBAY = False
		USER_AGENT 
		DEFAULT_REQUESTS_HEADERS # 不能写ua
	
		ITEM_PIPELINES
		SPIDER_MIDDLEWARES
		DOWNLOADER_MIDDLEWARES
			# 左位置，右权重，值越小，先执行
	
		COOKIES_DEBUG 默认False,True表示日志中显示cookies的传递
		COOKIES_ENBLE 默认True,表示开启cookies的传递的功能
	
		LOG_LEVEL 默认使DEBUG,表示日志输出的等级
			LOG_LEVEL = 'WARNING' # 只输出warning及以上等级的日志
		LOG_FILE= 指定日志文件的路径，如果指定日志文件路径终端将不再显示日志信息，同时受LOG_LEVEL的控制
	
	
		# scrapy_redis配置
		REDIS_URL
		DUPEFILTER_CLASS 
		SCHEDULER
		SCHEDULER_PERSIST 
		ITEM_PIPELINES={'scrapy_redis.pipelines.RedisPipeline': 400}
	
		# scrapy_splash配置
		SPLASH_URL
		DUPEFILTER_CLASS 
		DOWNLOADER_MIDDLEWARES = {
		    'scrapy_splash.SplashCookiesMiddleware': 723,
		    'scrapy_splash.SplashMiddleware': 725,
		    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
		}
		HTTPCACHE_STORAGE
	
		# scrapy_redis和scrapy_splash混合使用的配置
			# 即需要scrapy_redis的配置
			# 还需要scrapy_splash的配置
			# 因为DUPEFILTER_CLASS配置重复，需要指定重写的类
				from copy import deepcopy
				from scrapy.utils.request import request_fingerprint
				from scrapy.utils.url import canonicalize_url
				from scrapy_splash.utils import dict_hash
				from scrapy_redis.dupefilter import RFPDupeFilter
				def splash_request_fingerprint(request, include_headers=None):
				    fp = request_fingerprint(request, include_headers=include_headers)
				    if 'splash' not in request.meta: return fp
				    splash_options = deepcopy(request.meta['splash'])
				    args = splash_options.setdefault('args', {})
				    if 'url' in args:
				        args['url'] = canonicalize_url(args['url'], keep_fragments=True)
				    return dict_hash(splash_options, fp)
				class SplashAwareDupeFilter(RFPDupeFilter):
				    def request_fingerprint(self, request):
				        return splash_request_fingerprint(request)

2. scrapyd使用

   		scrapyd 开启scrapyd的服务
   		修改scrapy.cfg文件中的url
   		scrapyd-deploy -p 项目名
   	
   		http://host:6800/schedule.json
   			project
   			spider
   	
   		http://host:6800/cancel.json
   			project
   			job  # 启动时返回的jobid

3. 关于crontab定时任务工具中几个小点

   ```
   0 */2 * * * ls # 没过两个小时之后的00分执行一次ls
   	path/python xxx.py >> xxx.txt 2>&1
   	# 2>&1 表示把标准错误2当作标准输出1写入前边 xxx.txt文件
   ```

   ​